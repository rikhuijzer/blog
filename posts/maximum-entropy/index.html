<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

   <link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> The principle of maximum entropy - Huijzer.xyz </title> 
  

  <meta property="og:title" content="The principle of maximum entropy" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="Obtaining the least informative distribution." />
  <!-- <meta property="og:image" content="{{website_url}}{{requiredfill image}}" /> -->

  <meta name="twitter:title" content="The principle of maximum entropy" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:creator" content="@rikhuijzer"/>

  <script src="https://cdn.usefathom.com/script.js" data-site="BQRGKKNX" defer></script>
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/posts/">Blog</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      <img class="rss-icon" src="/assets/feed.svg">
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> The principle of maximum entropy </h1> 
   <span class="page-date"> 2020-09-26 </span> 
</div>
<div class="franklin-content">
<p>Say that you are a statistician and are asked to come up with a probability distribution for the current state of knowledge on some particular topic you know little about. &#40;This, in Bayesian statistics, is known as choosing a suitable prior.&#41; To do this, the safest bet is coming up with the least informative distribution via the <em>principle of maximum entropy</em>.</p>
<p>This principle is clearly explained by <span class="bibref"><a href="#jaynes1968">Jaynes (1968)</a></span>: consider a die which has been tossed a very large number of times \(N\). We expect the average to be \(3.5\), that is, we expect the following distribution where \(P_n = \frac{1}{6}\) for each \(n\).</p>
<pre><code class="language-julia">using AlgebraOfGraphics
using CairoMakie
using DataFrames

function plot_distribution&#40;probabilities::Array&#41;
  np &#61; mapping&#40;&#91;1:6&#93; &#61;&gt; :n, &#91;probabilities&#93; &#61;&gt; :P_n&#41; * visual&#40;BarPlot&#41;
  axis &#61; &#40;; xticks&#61;1:6, limits&#61;&#40;nothing, &#40;0, 1&#41;&#41;, height&#61;200&#41;
  draw&#40;np; axis&#41;
end</code></pre>

<pre><code class="language-julia">plot_distribution&#40;&#91;1/6, 1/6, 1/6, 1/6, 1/6, 1/6&#93;&#41;</code></pre>
<img src="/assets/posts/maximum-entropy/code/output/unbiased.svg" alt="">
<p>Instead, we are told that the average is \(4.5\). How likely is it for each number \(n = 1,2, \ldots, 6\) to come up for the next toss?</p>
<p>Since we know that \(P\) always sums to 1, we have</p>
\[ \sum_{n=1}^6 P_n = 1. \]
<p>We also know that the average is \(4.5\), that is,</p>
\[ \sum_{n=1}^6 n \cdot P_n = 4.5. \]
<p>We could satisfy these constraints by choosing \(P_4 = P_5 = \frac{1}{2}\).</p>
<pre><code class="language-julia">plot_distribution&#40;&#91;0, 0, 0, 0.5, 0.5, 0&#93;&#41;</code></pre>
<p><img src="/assets/posts/maximum-entropy/code/output/naive.svg" alt=""> This is unlikely to be the distribution for our data since it can be derived in relatively few ways, namely: by throwing only \(4\) and \(5\), and in such a way that the throws average to \(4.5\). A more likely distribution would be </p>
<pre><code class="language-julia">plot_distribution&#40;&#91;0, 0, 1/4, 1/4, 1/4, 1/4&#93;&#41;</code></pre>
<p><img src="/assets/posts/maximum-entropy/code/output/quarters.svg" alt=""> This is still not the least informative distribution since it assumes \(n = 1\) and \(n = 2\) to be impossible events. Jaynes presents the straight line solution \(P_n = (12n - 7)/210\),</p>
<pre><code class="language-julia">fg &#61; plot_distribution&#40;&#91;&#40;12n - 7&#41;/210 for n in 1:6&#93;&#41;
fg</code></pre>
<p><img src="/assets/posts/maximum-entropy/code/output/straight.svg" alt=""> This solution would also fail if the mean would have been higher, because then \(P_0 = 0\) would occur again. The correct measure is the following information measure <span class="bibref">(<a href="#shannon1948">Shannon, 1948</a>)</span>, which is also known as information entropy,</p>
\[ S_I = - \sum_i p_i \log p_i. \]
<p>We can find \(p_i\) for \(p_i = 1, 2, \ldots, 6\) by maximizing \(S_I\) for given constraints. This problem, known as <em>MaxEnt</em>, is hard to solve manually since there are \(6\) unknowns and various constraints. The solution can be approximated by rewriting it to a linear program.</p>
<p>Alternatively, analytic solutions exist for some subsets of this Shanon entropy maximization problem <span class="bibref">(<a href="#zabarankin2014">Zabarankin & Uryasev, 2014</a>)</span>. Here, we have that the mean is known &#40;and nothing else&#41;, so the number of moments \(m\) is \(1\). Then, the maximum entropy distribution takes the form &#40;<span class="bibref"><a href="#zabarankin2014">Zabarankin & Uryasev, 2014</a></span>; Eq. 5.1.7&#41;</p>
\[ P_n = \frac{e^{\rho n}}{\sum_{n=1}^6 e^{\rho n}}, \: \text{ for } n = 1, 2, ..., 6. \]
<p>This function satisfies \(\sum_{n=1}^6 P_n = 1\) for any \(\rho\). Now, we only have to find the \(\rho\) for which the average is \(4.5\). After some <a href="#trial-and-error">trial and error</a>, you&#39;ll find that \(\rho = 0.3715\) gives \(\sum_{n=1}^6 n \cdot P_n \approx 4.501\).</p>
<pre><code class="language-julia">plot_distribution&#40;&#91;0.0543, 0.0787, 0.114, 0.165, 0.240, 0.348&#93;&#41;</code></pre>
<p><img src="/assets/posts/maximum-entropy/code/output/entropy.svg" alt=""> This is the least informative distribution which satisfies the constraints. In other words, this is the distribution which can be obtained in the largest number of ways, given the constraints. For another example of maximum entropy distributions, see Chapter 10.1 of the book by <span class="bibref"><a href="#mcelreath2020">McElreath (2020)</a></span>.</p>
<h3 id="references"><a href="#references" class="header-anchor">References</a></h3>
<p><a id="jaynes1968" class="anchor"></a> Jaynes, E. T. &#40;1968&#41;. Prior Probabilities. IEEE Transactions on Systems Science and Cybernetics. 4 &#40;3&#41;: 227â€“241. <a href="https://doi.org/10.1109/TSSC.1968.300117">https://doi.org/10.1109/TSSC.1968.300117</a></p>
<p><a id="mcelreath2020" class="anchor"></a> McElreath, R. &#40;2020&#41;. Statistical Rethinking: A Bayesian course with examples in R and Stan. CRC press.</p>
<p><a id="shannon1948" class="anchor"></a> Shannon, C. E. &#40;1948&#41;. A mathematical theory of communication. The Bell System Technical Journal &#40;Volume: 27 , Issue: 3 , July 1948&#41;. <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">https://doi.org/10.1002/j.1538-7305.1948.tb01338.x</a></p>
<p><a id="zabarankin2014" class="anchor"></a> Zabarankin M., Uryasev S. &#40;2014&#41; Entropy Maximization. In: Statistical Decision Problems. Springer Optimization and Its Applications, vol 85. Springer, New York, NY. <a href="https://doi.org/10.1007/978-1-4614-8471-4_5">https://doi.org/10.1007/978-1-4614-8471-4_5</a></p>
<h2 id="trial_and_error"><a href="#trial_and_error" class="header-anchor">Trial and error</a></h2>
<pre><code class="language-julia">julia &gt; p&#40;k, rho&#41; &#61; exp&#40;rho*k&#41; / sum&#40;&#91;exp&#40;rho*1&#41;, exp&#40;rho*2&#41;, exp&#40;rho*3&#41;, exp&#40;rho*4&#41;, exp&#40;rho*5&#41;, exp&#40;rho*6&#41;&#93;&#41;
julia &gt; function ps&#40;rho&#41;
    values &#61; map&#40;k -&gt; p&#40;k, rho&#41;, 1:6&#41;
    @show values
    sum_values &#61; sum&#40;values&#41;
    @show sum_values
    average &#61; sum&#40;&#91;values&#91;1&#93;*1, values&#91;2&#93;*2, values&#91;3&#93;*3, values&#91;4&#93;*4, values&#91;5&#93;*5, values&#91;6&#93;*6&#93;&#41;
    @show average
    nothing
end

julia&gt; ps&#40;0.4&#41;
values &#61; &#91;0.04906874617024226, 0.0732019674190579, 0.1092045029116822, 0.16291397453728548, 0.24303909080562353, 0.36257171815610867&#93;
sum_values &#61; 1.0
average &#61; 4.565367850857316

julia&gt; ps&#40;0.34&#41;
values &#61; &#91;0.0605247711421319, 0.08503413138555115, 0.11946849800579816, 0.16784697842149762, 0.23581620791666263, 0.3313094131283586&#93;
sum_values &#61; 1.0
average &#61; 4.427323959970084

...

julia&gt; ps&#40;0.3715&#41;
values &#61; &#91;0.05426741458481561, 0.07868275019416264, 0.11408273685935422, 0.165409455277101, 0.2398284670256302, 0.3477291760589363&#93;
sum_values &#61; 1.0
average &#61; 4.501036338141376</code></pre>
<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Rik Huijzer. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2021-11-10.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>
  const options = {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  };
  renderMathInElement(document.body, options);
</script>

    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
