<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
 
   <link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> Frequentist and Bayesian coin flipping - Huijzer.xyz </title> 
  

  <meta property="og:title" content="Frequentist and Bayesian coin flipping" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="Comparing both statistical paradigms on a coin flipping example." />
  <meta property="og:image" content="https://huijzer.xyz/assets/og-image/frequentist-bayesian-coin-flipping.png" />

  <meta name="twitter:title" content="Frequentist and Bayesian coin flipping" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:creator" content="@rikhuijzer"/>

  <script src="https://cdn.usefathom.com/script.js" data-site="BQRGKKNX" defer></script>
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/posts/">Blog</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      <img class="rss-icon" src="/assets/feed.svg">
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> Frequentist and Bayesian coin flipping </h1> 
   <span class="page-date"> 2020-11-14 </span> 
</div>
<div class="franklin-content">
<p>To me, it is still unclear what exactly is the difference between Frequentist and Bayesian statistics. Most explanations involve terms such as &quot;likelihood&quot;, &quot;uncertainty&quot; and &quot;prior probabilities&quot;. Here, I&#39;m going to show the difference between both statistical paradigms by using a coin flipping example. In the examples, the effect of showing more data to both paradigms will be visualised.</p>
<div class="franklin-toc"><ol><li><a href="#generating_data">Generating data</a></li><li><a href="#calculate_probability_estimates">Calculate probability estimates</a></li><li><a href="#conclusion">Conclusion</a></li><li><a href="#references">References</a></li></ol></div>
<h2 id="generating_data"><a href="#generating_data" class="header-anchor">Generating data</a></h2>
<p>Lets start by generating some data from a fair coin flip, that is, the probability of heads is 0.5.</p>


<pre><code class="language-julia">using Distributions
using Random

n &#61; 80
Random.seed&#33;&#40;102&#41;
p_true &#61; 0.5
is_heads &#61; rand&#40;Bernoulli&#40;p_true&#41;, n&#41;</code></pre>
<p>To give some intuition about the sample, the first six elements of <code>is_heads</code> are</p>
<pre><code class="plaintext code-output">is_heads[1:6] = Bool[1, 0, 0, 1, 1, 1]
</code></pre>
<h2 id="calculate_probability_estimates"><a href="#calculate_probability_estimates" class="header-anchor">Calculate probability estimates</a></h2>
<p>The Frequentist estimate for a one sample t-test after seeing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span> samples can be calculated with</p>
<pre><code class="language-julia">using HypothesisTests

function frequentist_estimate&#40;n&#41;
  t_result &#61; OneSampleTTest&#40;is_heads&#91;1:n&#93;&#41;
  t_lower, t_upper &#61; confint&#40;t_result&#41;
  &#40;lower &#61; t_lower, middle &#61; t_result.xbar, upper &#61; t_upper&#41;
end</code></pre>
<p>For the Bayesian estimate, we can use the closed-form solution <span class="bibref">(<a href="#turing2020closed">The Turing Language, 2020</a>)</span>. A closed-form solution is not available for many real-world problems, but quite useful for this example.</p>
<pre><code class="language-julia">closed_form_prior &#61; Beta&#40;1, 1&#41;
function update_belief&#40;k&#41;
  heads &#61; sum&#40;is_heads&#91;1:k-1&#93;&#41;
  tails &#61; k - heads
  updated_belief &#61; Beta&#40;closed_form_prior.α &#43; heads, closed_form_prior.β &#43; tails&#41;
end
beliefs &#61; &#91;closed_form_prior; update_belief.&#40;1:n&#41;&#93;</code></pre>

<pre><code class="language-julia">function bayesian_estimate&#40;n&#41;
  distribution &#61; beliefs&#91;n&#93;
  q&#40;x&#41; &#61; quantile&#40;distribution, x&#41;
  &#40;lower &#61; q&#40;0.025&#41;, middle &#61; mean&#40;distribution&#41;, upper &#61; q&#40;0.975&#41;&#41;
end</code></pre>
<pre><code class="language-julia">using AlgebraOfGraphics
using CairoMakie

function plot_estimates&#40;estimate_function; title&#61;&quot;&quot;&#41;
  draws &#61; 2:4:80
  estimates &#61; estimate_function.&#40;draws&#41;
  middles &#61; &#91;t.middle for t in estimates&#93;
  lowers &#61; &#91;t.lower for t in estimates&#93;
  uppers &#61; &#91;t.upper for t in estimates&#93;
  df &#61; &#40;; draws, estimates, P&#61;middles&#41;
  layers &#61; data&#40;df&#41; * visual&#40;Scatter&#41;
  df_middle &#61; &#40;; P&#61;fill&#40;0.5, length&#40;draws&#41; &#43; 2&#41;, draws&#61;&#91;-1; draws; 83&#93;&#41;
  layers &#43;&#61; data&#40;df_middle&#41; * visual&#40;Lines&#41; * visual&#40;linestyle&#61;:dash&#41;
  for &#40;n, lower, upper&#41; in zip&#40;draws, lowers, uppers&#41;
    df_bounds &#61; &#40;; P&#61;&#91;lower, upper&#93;, draws&#61;&#91;n, n&#93;&#41;
    layers &#43;&#61; data&#40;df_bounds&#41; * visual&#40;Lines&#41;
  end

  axis &#61; &#40;; yticks&#61;0:20:80, limits&#61;&#40;&#40;-0.2, 1.2&#41;, nothing&#41;, title&#41;
  map &#61; mapping&#40;:P &#61;&gt; &quot;Probability of heads&quot;, :draws &#61;&gt; &quot;Observed number of draws&quot;&#41;
  draw&#40;layers * map; axis&#41;
end</code></pre>

<pre><code class="language-julia">fg &#61; plot_estimates&#40;frequentist_estimate; title&#61;&quot;Frequentist estimates&quot;&#41;
fg</code></pre>
<img src="/assets/posts/frequentist-bayesian-coin-flipping/code/output/frequentist-estimates.svg" alt="">
<pre><code class="language-julia">fg &#61; plot_estimates&#40;bayesian_estimate; title&#61;&quot;Bayesian estimates&quot;&#41;
fg</code></pre>
<img src="/assets/posts/frequentist-bayesian-coin-flipping/code/output/bayesian-estimates.svg" alt="">
<h2 id="conclusion"><a href="#conclusion" class="header-anchor">Conclusion</a></h2>
<p>Based on these plots, we can conclude two things. Firstly, the Bayesian approach provides better estimates for small sample sizes. The Bayesian approach successfully uses the fact that a probability should be between 0 and 1, which was given to the model via the <code>Beta&#40;1, 1&#41;</code> prior. For increasingly larger sample sizes, the difference between both statistical paradigms vanish in this situation. Secondly, collecting more and more samples until the result is significant is dangerous. This approach is called <em>optional stopping</em>. Around 25 samples, it would find that the data must come from a distribution with a mean higher than 0.5, whereas we know that this is false. <span class="bibref"><a href="#tcumming2011">Cumming (2011)</a></span> calls this the &quot;dance of the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span>-values&quot;.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<p><a id="tcumming2011" class="anchor"></a> Cumming, G. &#40;2011&#41;. Understanding the new statistics: Effect sizes, confidence intervals, and meta-analysis.  Routledge.</p>
<p><a id="turing2020closed" class="anchor"></a> The Turing Language &#40;2020&#41;. Introduction to Turing. <a href="https://turing.ml/dev/tutorials/0-introduction/">https://turing.ml/dev/tutorials/0-introduction/</a>.</p>
<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Rik Huijzer. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2021-09-08.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    
        



    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
