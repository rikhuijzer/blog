<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

   <link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> Increasing model accuracy by using foreknowledge - Huijzer.xyz </title> 
  

  <meta property="og:title" content="Increasing model accuracy by using foreknowledge" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="Using priors for binary logistic regression" />
  <!-- <meta property="og:image" content="{{website_url}}{{requiredfill image}}" /> -->

  <meta name="twitter:title" content="Increasing model accuracy by using foreknowledge" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:creator" content="@rikhuijzer"/>

  <script src="https://cdn.usefathom.com/script.js" data-site="BQRGKKNX" defer></script>
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/posts/">Blog</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      <img class="rss-icon" src="/assets/feed.svg">
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> Increasing model accuracy by using foreknowledge </h1> 
   <span class="page-date"> 2021-06-16 </span> 
</div>
<div class="franklin-content">
</p>
<p>

<div class="markdown"><p>Typically, when making predictions via a linear model, we fit the model on our data and make predictions from the fitted model. However, this doesn&#39;t take much foreknowledge into account. For example, when predicting a person&#39;s length given only the weight and gender, we already have an intuition about the effect size and direction. Bayesian analysis should be able to incorporate this prior information.</p>
<p>In this blog post, I aim to figure out whether foreknowledge can, in theory, increase model accuracy. To do this, I generate data and fit a linear model and a Bayesian binary regression. Next, I compare the accuracy of the model parameters from the linear and Bayesian model.</p>
</div>

<pre><code class="language-julia">begin
	using AlgebraOfGraphics
	using CairoMakie
	using CategoricalArrays
	using DataFrames
	using GLM
	using MLDataUtils: rescale!
	using Random: seed!
	using Statistics
	using StatsFuns
	using Turing
end</code></pre>
<pre><code class="code-output">nothing</code></pre>


<pre><code class="code-output">nothing</code></pre>


<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><p>We define the model as <span class="tex">$g_i &#61; a_e * a_i &#43; r_e * r_i &#43; \epsilon_i &#61; 1.1 * a_i &#43; 1.05 * r_i &#43; \epsilon_i$</span> where <span class="tex">$a_e$</span> is the coefficient for the age, <span class="tex">$r_e$</span> is a coefficient for the nationality and <span class="tex">$\epsilon_i$</span> is some random noise for individual <span class="tex">$i$</span>.</p>
<p>We generate data for <span class="tex">$n$</span> individuals via:</p>
</div>

<pre><code class="language-julia">function generate_data(i::Int)
  seed!(i)

  n = 120
  I = 1:n
  P = [i % 2 == 0 for i in I]
  r_2(x) = round(x; digits=2)

  A = r_2.([p ? rand(Normal(aₑ * 18, 1)) : rand(Normal(18, 1)) for p in P])
  R = r_2.([p ? rand(Normal(rₑ * 6, 3)) : rand(Normal(6, 3)) for p in P])
  E = r_2.(rand(Normal(0, 1), n))
  G = aₑ .* A + rₑ .* R .+ E
  G = r_2.(G)

  df = DataFrame(age=A, recent=R, error=E, grade=G, pass=P)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">df = generate_data(1)</code></pre>
<pre><code class="code-output">nothing</code></pre>


<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h2>Linear regression</h2>
<p>First, we fit a linear model and verify that the coefficients are estimated reasonably well. Here, the only prior information that we give the model is the structure of the data, that is, a formula.</p>
</div>

<pre><code class="language-julia">linear_model = lm(@formula(grade ~ age + recent), df)</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">r5(x) = round(x; digits=5)</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">coefa = coef(linear_model)[2] |&gt; r5</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">coefr = coef(linear_model)[3] |&gt; r5</code></pre>
<pre><code class="code-output">nothing</code></pre>


<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h2>Bayesian regression</h2>
<p>For the Bayesian regression we fit a model via Turing.jl. Now, we give the model information about the structure of the data as well as priors for the size of the coefficients. For demonstration purposes, I&#39;ve set the priors to the correct values. This is reasonable because I was wondering whether finding a good prior could have a positive effect on the model accuracy.</p>
</div>

<pre><code class="language-julia">function rescale_data(df)
    out = DataFrame(df)
    rescale!(out, [:age, :recent, :grade])
    out
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">rescaled = let
	rescaled = rescale_data(df)
	rescaled[!, :pass_num] = [p ? 1.0 : 0.0 for p in rescaled.pass]
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">@model function bayesian_model(ages, recents, grades, n)
    intercept ~ Normal(0, 5)
    βₐ ~ Normal(aₑ, 1)
    βᵣ ~ Normal(rₑ, 3)
    σ ~ truncated(Cauchy(0, 2), 0, Inf)

    μ = intercept .+ βₐ * ages .+ βᵣ * recents
    grades ~ MvNormal(μ, σ)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">chns = let
	n = nrow(df)
	bm = bayesian_model(df.age, df.recent, df.grade, n)
	chns = Turing.sample(bm, NUTS(), MCMCThreads(), 10_000, 3)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><p>Let&#39;s plot the density for the coefficient estimates <span class="tex">$\beta_a$</span> and <span class="tex">$\beta_r$</span>:</p>
</div>


<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><p>and compare the outputs from both models:</p>
</div>


<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h2>Conclusion</h2>
<p>After giving the true coefficients to the Bayesian model in the form of priors, it does score better than the linear model. However, the differences aren&#39;t very big. This could be due to the particular random noise in this sample <code>E</code> or due to the relatively big sample size. The more samples, the more likely it is that the data will overrule the prior. In any way, there are real-world situations where gathering extra data is more expensive than gathering priors via reading papers. In those cases, the increased accuracy introduced by using priors could have serious benefits.</p>
</div>


<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Rik Huijzer. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2021-11-09.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>
  const options = {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  };
  renderMathInElement(document.body, options);
</script>

    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
