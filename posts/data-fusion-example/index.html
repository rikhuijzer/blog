<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

   <link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> A data-fusion example - Huijzer.xyz </title> 
  

  <meta property="og:title" content="A data-fusion example" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="An example in causal data-fusion." />
  <!-- <meta property="og:image" content="{{website_url}}{{requiredfill image}}" /> -->

  <meta name="twitter:title" content="A data-fusion example" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:creator" content="@rikhuijzer"/>

  <script src="https://cdn.usefathom.com/script.js" data-site="BQRGKKNX" defer></script>
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/posts/">Blog</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      <img class="rss-icon" src="/assets/feed.svg">
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> A data-fusion example </h1> 
   <span class="page-date"> 2020-11-02 </span> 
</div>
<div class="franklin-content">


<p>In the blog post on <a href="/posts/correlations">correlations</a>, I ended with the well-known mantra: &quot;correlation does not imply causation&quot;. Most of empirical science has been focussing on the correlations, because determining causation is hard. However, <span class="bibref"><a href="#pearl2018why">Pearl & Mackenzie (2006)</a></span> urges scientists to search for mechanisms, because it &quot;is critical for science, as well as to everyday life, because different mechanisms call for different actions when circumstances change&quot;. For example, we know that a lack of Vitamin C causes us to get scurvy, or not. We can depict this as Vitamin C \(\rightarrow\) scurvy. Knowing this is very important, because without it we might think that bananas can cure scurvy <span class="bibref">(<a href="#pearl2018why">Pearl & Mackenzie (2006)</a>)</span> and send a trade ship on its way with bananas instead of oranges. Alternatively, we could have found that ships with strawberries don&#39;t get scurvy &#40;strawberries contain Vitamin C&#41; and conclude that all red fruits can cure scurvy.</p>
<p>So, it is important to know the exact cause of things to make good decisions. One of the easiest ways to determine cause and effect is via doing an intervention, denoted by the <em>do</em>-operator <span class="bibref">(<a href="#pearl2009">Pearl (2009)</a>)</span>. Looking back on the scurvy example, it can be rewritten as follows: let Vitamin C be denoted by \(X\) &#40;the cause&#41; and scurvy be denoted by \(Y\) &#40;the effect&#41;. Then, we test whether scurvy is caused by a lack of Vitamin C by figuring out \(P(Y | do(X))\). This <em>do</em>-operator means that you would do an experiment where you change \(X\) and only \(X\) and then measure the effect on \(Y\). The &quot;and only&quot; part is important and can be obtained by doing a randomized controlled trial. The idea of such a trial is to take two completely random subsets of your sample. Then, for you subset you change \(X\) and for the other subset you do not change \(X\). Apart from \(X\), you don&#39;t try to change anything, so in medical settings the doctors are not even allowed who gets the medication since that could affect \(Y\).  &#40;See the story of <a href="https://simple.wikipedia.org/wiki/Clever_Hans">Clever Hans</a> for an example where observers affect the outcome by accident.&#41; If done correctly, a randomized trial removes the incoming causal arrows. For example, without using randomization, it could be that the patients who get the medicin are actually cured by swimming in the sea \(Z\). We can depict this in a causal graph as</p>


</div>
<div class="tikz">
  <img src="/assets/posts/data-fusion-example/code/output/confounder.svg" style="width:150px" />
</div>
<div class="franklin-content">

<p>&#40;Thanks to <span class="bibref"><a href="#kumor2020">Kumor (2018)</a></span> for code examples of these graphs.&#41; If the randomization is representative of the whole population and executed correctly, then we can say that the graph changed to</p>


</div>
<div class="tikz">
  <img src="/assets/posts/data-fusion-example/code/output/controlled.svg" style="width:150px" />
</div>
<div class="franklin-content">

<p>which assumes that the two subsets are not fundamentally different. Unfortunately, it is often not possible to apply a randomized trial since it could be too expensive, too time consuming or unethical. For instance, it is a bad plan to to split a group in half and force one half of the group to smoke while tracking the group for 40 years to see whether smoking causes cancer. Luckily, solutions exist to determine causation from observations alone <span class="bibref">(<a href="#bollen2013">Bollen & Pearl (2013)</a>)</span>. Even better, <span class="bibref"><a href="#bareinboim2016">Bareinboim & Pearl (2016)</a></span> summarizes how data from observational studies can be combined with randomized trials to find cause and effect; this can be useful to learn a causal effect about one population and apply it to another population. This problem is similar to meta-analyses. However, meta-analyses typically &quot;&#39;&#91;average&#93; out&#39; differences &#40;e.g., using inverse-variance weighting&#41;, which, in general, tends to blur, rather than exploit design distinctions among the available studies&quot; <span class="bibref">(<a href="#bareinboim2016">Bareinboim & Pearl (2016)</a>)</span>. For a longer discussion about the lack of effectiveness of meta-analyses, see <span class="bibref"><a href="#vrieze2018">de Vrieze (2018)</a></span>.</p>
<p>In this blog post, my aim is to look at some examples of combining observational and randomized controlled trail data in an attempt to figure out how and when it can be applied.</p>
<h2 id="do-calculus"><a href="#do-calculus" class="header-anchor">Do-Calculus</a></h2>
<p>Thanks to the <em>do</em>-calculus presented by <span class="bibref">(<a href="#pearl2009">Pearl (2009)</a>)</span>, we can rewrite our graph without thinking.  &#40;By that, I mean that the calculus allows you to easily verify equivalences like \(a + b = b + a\) for any \(a\) and \(b\).&#41; For arbitrary disjoint sets of nodes \(X, Y, Z\) and \(W\) in a causal DAG \(G\), with \(G_{\overline{X}}\)  and \(G_{\underline{X}}\) denoting the graph obtained by, respectively, deleting all arrows pointing to and emerging from nodes in \(G\), we have <span class="bibref">(<a href="#bareinboim2016">Bareinboim & Pearl (2016)</a>)</span></p>
<p><em>Rule 1</em> &#40;insertion/deletion of observations&#41;:</p>
\[ P(y|do(x),z,w) = P(y|do(x),w) \: \text{if} \: (Y \perp Z|X, W)_{G_{\overline{X}}}. \]
<p><em>Rule 2</em> &#40;action/observation exchange&#41;:</p>
\[ P(y|do(x), do(z), w) = P(y|do(x), z, w) \: \text{if} \: (Y \perp Z|X, W)_{G_{\overline{X} \underline{Z}}}. \]
<p><em>Rule 3</em> &#40;insertion/deletion of actions&#41;:</p>
\[ P(y|do(x), do(z), w) = P(y|do(x), w) \: \text{if} \: (Y \perp Z|X, W)_{G_{\overline{X} \overline{Z^*}}}. \]
<h2 id="transportability"><a href="#transportability" class="header-anchor">Transportability</a></h2>
<p>Scientific results are meant to be used across different populations. This, according to <span class="bibref"><a href="#pearl2014external">Pearl & Bareinboim (2014)</a></span>, is called <em>transportability</em>. Specifically, it is about transfering causal effects from experimental studies to observational studies. Here, I will mostly copy the example of <span class="bibref">(<a href="#bareinboim2016">Bareinboim & Pearl (2016)</a>)</span> and add some clarifications at the steps which I find unclear. Consider an experimental source \(\pi\) and an observational target \(\pi^*\) population. The variables are treatment \(X\), outcome \(Y\), age \(Z\) and a set of unaccounted factors \(S\).</p>


</div>
<div class="tikz">
  <img src="/assets/posts/data-fusion-example/code/output/first.svg" style="width:150px" />
</div>
<div class="franklin-content">

<p>The unaccounted factors create differences in age between \(\pi\) and \(\pi^*\). These unaccounted factors are unknown, but it is known that they cause the differences in age between \(\pi\) and \(\pi^*\); that is why \(S\) is denoted with a black box in the graph. Note that this means that the graph is an overlapping of the graph of the source and target population. Now, the query can be rewritten to <span class="bibref">(<a href="#bareinboim2016">Bareinboim & Pearl (2016)</a>)</span></p>
\[
\begin{aligned}
& Q \\
= \hspace{3mm} & \hspace{5mm} \{ \: \text{ By definition of $Q$. \: } \} \\
 & \sum_z P(y|do(x), S=s^*,z)P(z|S=s^*,do(x)) \\
= \hspace{3mm} & \hspace{5mm} \{ \: \text{By \textit{S}-admissibility. \: } \} \\
 & \sum_z P(y|do(x), z)P(z|S=s^*,do(x)) \\
= \hspace{3mm} & \hspace{5mm} \{ \: \text{By the 3rd rule of the \textit{do}-calculus.} \: \} \\
& \sum_z P(y|do(x), z) P(z|S = s^*) \\
= \hspace{3mm} & \hspace{5mm} \{ \: \text{By definition of the \textit{S}-node.} \: \} \\
& \sum_z P(y|do(x), z) P^*(z). 
\end{aligned}
\]
<p>This is called a <em>transport formula</em>, because it explains &quot;how experimental findings in \(\pi\) are transported over to \(\pi*\); the first factor is estimable from \(\pi\) and the second one from \(\pi^*\)&quot; <span class="bibref">(<a href="#bareinboim2016">Bareinboim & Pearl (2016)</a>)</span>.</p>
<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<p><a id="bareinboim2016" class="anchor"></a> Bareinboim &amp; Pearl. &#40;2016&#41;. Causal inference and the data-fusion problem. <a href="https://doi.org/10.1073/pnas.1510507113">https://doi.org/10.1073/pnas.1510507113</a>.</p>
<p><a id="bollen2013" class="anchor"></a> Bollen &amp; Pearl. &#40;2013&#41;. Eight Myths About Causality and Structural Equation Models. <a href="https://doi.org/10.1007/978-94-007-6094-3_15">https://doi.org/10.1007/978-94-007-6094-3_15</a>.</p>
<p><a id="kumor2020" class="anchor"></a> Kumor, D. &#40;2018&#41;. Causal Graphs in LaTeX. <a href="https://dkumor.com/posts/technical/2018/08/15/causal-tikz/">https://dkumor.com/posts/technical/2018/08/15/causal-tikz/</a>.</p>
<p><a id="pearl2009" class="anchor"></a>  Pearl, J. &#40;2009&#41;. Causality. Cambridge university press. <a href="https://doi.org/10.1017/CBO9780511803161">https://doi.org/10.1017/CBO9780511803161</a>.</p>
<p><a id="pearl2014external" class="anchor"></a> Pearl &amp; Bareinboim. &#40;2014&#41;. External Validity: From Do-Calculus to Transportability Across Populations. Statistical Science. <a href="https://doi.org/10.1214/14-STS486">https://doi.org/10.1214/14-STS486</a>. <a id="pearl2018why" class="anchor"></a>  Pearl, J., &amp; Mackenzie, D. &#40;2018&#41;.  The book of why: the new science of cause and effect.  Basic Books.</p>
<p><a id="vrieze2018" class="anchor"></a> de Vrieze, J. &#40;2018, September 18&#41;. Meta-analyses were supposed to end scientific debates. Often, they only cause more controversy. Science Magazine. <a href="https://www.sciencemag.org/news/2018/09/meta-analyses-were-supposed-end-scientific-debates-often-they-only-cause-more">https://www.sciencemag.org/news/2018/09/meta-analyses-were-supposed-end-scientific-debates-often-they-only-cause-more</a>. Accessed on 2020-11-01.</p>
<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Rik Huijzer. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2021-11-10.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>
  const options = {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  };
  renderMathInElement(document.body, options);
</script>

    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
