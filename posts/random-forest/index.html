<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/libs/katex/katex.min.css">

   <link rel="stylesheet" href="/libs/highlight/github.min.css">
 
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title> Random forest classification in Julia - Huijzer.xyz </title> 
  

  <meta property="og:title" content="Random forest classification in Julia" />
  <meta property="og:type" content="article" /> 
  <meta property="og:description" content="Fitting a random forest classifier and reporting accuracy metrics." />
  <!-- <meta property="og:image" content="{{website_url}}{{requiredfill image}}" /> -->

  <meta name="twitter:title" content="Random forest classification in Julia" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:creator" content="@rikhuijzer"/>

  <script src="https://cdn.usefathom.com/script.js" data-site="BQRGKKNX" defer></script>
</head>
<body>
  <header>
<div class="blog-name"><a href="/">HUIJZER.XYZ</a></div>
<nav>
  <ul>
    <li><a href="/about/">About</a></li>
    <li><a href="/posts/">Blog</a></li>
    <li><a type="application/rss+xml" href="https://huijzer.xyz/feed.xml">
      <img class="rss-icon" src="/assets/feed.svg">
      </a></li>
  </ul>
</nav>
</header>


<div class="franklin-content">
   <h1 class="page-title"> Random forest classification in Julia </h1> 
   <span class="page-date"> 2021-01-21 </span> 
</div>
<div class="franklin-content">
<p>Below is example code for fitting and evaluating a linear regression and random forest classifier in Julia. I&#39;ve used both models to have a baseline for the random forest. The model is evaluated on a mock variable \(U\) generated from two distributions, namely</p>
\[
\begin{aligned}
d_1 &= \text{Normal}(\mu_1, \sigma) \: \: \text{and} \\
d_2 &= \text{Normal}(\mu_2, \sigma),
\end{aligned}
\]
<p>where \(\mu_1 = 10\), \(\mu_2 = 12\) and \(\sigma = 2\). The random variable \(V\) is just noise meant to fool the classifier.</p>
<p>This data isn&#39;t meant to show that random forests are good classifiers. One way to do that would be to have more variables than observations <span class="bibref">(<a href="#pbiau2016">Biau & Scornet, 2016</a>)</span>.</p>
</p>
<p>

<div class="markdown"><h1>Data generation</h1>
</div>

<pre><code class="language-julia">begin
	import MLDataUtils
	import MLJGLMInterface
	import MLJDecisionTreeInterface

	using AlgebraOfGraphics
	using CairoMakie
	using CategoricalArrays
	using DataFrames
	using Distributions
	using MLJBase
	using MLJ
	using StableRNGs: StableRNG
	using Random
end</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">df = let
	n = 80
	μ1 = 10
	μ2 = 12
	σ = 2

	d1 = Normal(μ1, σ)
	d2 = Normal(μ2, σ)

	Random.seed!(123)
	classes = categorical(rand(["A", "B"], n))

	df = DataFrame(
		class = categorical(classes),
		U = [class == "A" ? rand(d1) : rand(d2) for class in classes],
		V = rand(Normal(100, 10), n)
	)
end</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">let
	uv = data(df) * mapping(:U, :V, color=:class)
	draw(uv)
end</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h1>Train and test split</h1>
<p>Training and evaluating &#40;testing&#41; on the same data is not particulary useful because we want to know how well our model generalizes. For more information, see topics such as <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>. Instead, we split the data up in a train and test set.</p>
</div>

<pre><code class="language-julia">train, test = let
	rng = StableRNG(123)
	MLJ.partition(eachindex(df.class), 0.7; shuffle=true, rng)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h1>Model fitting</h1>
</div>

<pre><code class="language-julia">logistic_model = let
	LinearBinary = @load LinearBinaryClassifier pkg=GLM verbosity=0
	logistic_model = LinearBinary()
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">begin
	logistic = machine(logistic_model, (U = df.U, V = df.V), df.class)
	fit!(logistic; rows=train)
	fitted_params(logistic).coef
end</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><p>The second coefficient in the linear model is close to zero. This is exactly what the model should do since <code>V</code> is random noise.</p>
</div>

<pre><code class="language-julia">forest_model = let
	DecisionTree = @load DecisionTreeClassifier pkg=DecisionTree verbosity=0
	tree = DecisionTree()
	EnsembleModel(atom=tree, n=10)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">forest = let
	forest = machine(forest_model, (U = df.U, V = df.V), df.class)
	fit!(forest; rows=train);
	forest
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h1>Accuracy</h1>
<p>Now that we have fitted the two models, we can compare the accuracies and plot the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operating characteristic</a>.</p>
</div>

<pre><code class="language-julia">let
	truths = df.class[test]
	
	logistic_predictions = MLJ.predict(logistic, rows=test)
	logistic_fprs, logistic_tprs, _ = roc_curve(logistic_predictions, truths)
	
	forest_predictions = MLJ.predict(forest, rows=test)
	forest_fprs, forest_tprs, _ = roc_curve(forest_predictions, truths)
	
	logistic_df = DataFrame(
		x = logistic_fprs,
		y = logistic_tprs,
		method = "logistic"
	)

	forest_df = DataFrame(
		x = forest_fprs,
		y = forest_tprs,
		method = "forest"
	)

	roc_df = vcat(logistic_df, forest_df)

	xy = data(roc_df)
	xy *= smooth() + visual(Scatter)
	xy *= mapping(
		:x =&gt; "False positive rate",
		:y =&gt; "True positive rate",
		color=:method)

	draw(xy)
end</code></pre>
<pre><code class="code-output">nothing</code></pre>


<div class="markdown"><h1>K-fold cross-validation</h1>
<p>By doing a train and test split, we basically threw a part of the data away. For small datasets, like the dataset in this example, that is not very efficient.  Therefore, we also do a <a href="https://en.wikipedia.org/wiki/Cross-validation_&#40;statistics&#41;#k-fold_cross-validation">k-fold cross-validation</a>.</p>
</div>

<pre><code class="language-julia">folds = let
	Random.seed!(123)
	rng = MersenneTwister(123)
	indexes = shuffle(rng, eachindex(df.class))
	folds = MLDataUtils.kfolds(indexes, k = 8)
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">r3(x) = round(x; digits=3);</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">function fitted_accuracy(model, train, test)
    forest = machine(model, (U = df.U, V = df.V), df.class)
    fit!(forest; rows=train)
    predictions = predict_mode(forest, rows=test)
    return accuracy(predictions, df.class[test]) |&gt; r3
end;</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">let 
	accuracies = [fitted_accuracy(logistic_model, train, test) for (train, test) in folds]
	accuracies, mean(accuracies) |&gt; r3
end</code></pre>
<pre><code class="code-output">nothing</code></pre>

<pre><code class="language-julia">let
	accuracies = [fitted_accuracy(forest_model, train, test) for (train, test) in folds]
	accuracies, mean(accuracies) |&gt; r3
end</code></pre>
<pre><code class="code-output">nothing</code></pre>


<h2 id="references"><a href="#references" class="header-anchor">References</a></h2>
<p><a id="pbiau2016" class="anchor"></a> Biau, G., Scornet, E. &#40;2016&#41;. A Random Forest Guided Tour. TEST 25, 197–227 &#40;2016&#41;. <a href="https://doi.org/10.1007/s11749-016-0481-7">https://doi.org/10.1007/s11749-016-0481-7</a></p>
<div class="page-foot">
  <div class="copyright">
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Rik Huijzer. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
    Last update: 2021-11-09.
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
    <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>
  const options = {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\begin{equation}", right: "\\end{equation}", display: true},
      {left: "\\begin{align}", right: "\\end{align}", display: true},
      {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
      {left: "\\begin{gather}", right: "\\end{gather}", display: true},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  };
  renderMathInElement(document.body, options);
</script>

    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
